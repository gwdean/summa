---
title: "MaxEnt"
author: "GW Dean"
date: "2023-01-06"
output: github_document
---
Notes on Maximum Entropy. Adapted from McElreath-2019, Ch. 10

"The principle of maximum entropy applies this measure of uncertainty [Shannon's] to the problem of choosing among probability distributions. Perhaps the simplest way to state the maximum entropy principle is:

> The distribution that can happen the most ways is also the  distribution with the biggest information entropy. The distribution with the biggest entropy is the most conservative distribution that obeys its constraints.

```{r}
p <- list()
p$A <- c(0, 0, 10, 0, 0)
p$B <- c(0, 1, 8, 1, 0)
p$C <- c(0, 2, 6, 2, 0)
p$D <- c(1, 2, 4, 2, 1)
p$E <- c(2, 2, 2, 2, 2)

p_norm <- lapply(p, function(q) q / sum(q))

(H <- sapply(p_norm, function(q) -sum(ifelse(q==0, 0, q*log(q)))))

ways <- c(1, 90, 1260, 37800, 113400)
logwayspp <- log(ways) / 10

# build list of candidate distributions
p <- list()
p[[1]] <- c(1/4, 1/4, 1/4, 1/4)
p[[2]] <- c(2/6, 1/6, 1/6, 2/6)
p[[3]] <- c(1/6, 2/6, 2/6, 1/6)
p[[4]] <- c(1/8, 4/8, 2/8, 1/8)

# compute expected value of each
sapply(p, function(p) sum(p * c(0, 1, 1, 2)) )

# compute entropy of each distribution
sapply(p, function(p) -sum(p * log(p)))
```
